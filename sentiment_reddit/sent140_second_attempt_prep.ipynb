{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment 140 Dataset\n",
    "## Data Prep\n",
    "#### Let's see what happens if we increase the dataset that we have. Let's use this 1.4Million sentences dataset and build a model with it.\n",
    "#### You can download the dataset from [Sentiment140](http://help.sentiment140.com/for-students). These are the tweet data they collected.\n",
    "#### THIS IS TOO SLOW ON CPU, I'LL PUT INSTRUCTIONS FOR RUNNING ON GPU\n",
    "\n",
    "We'll run all these cells once, so that data is preprocessed and can be used for training the actual model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WordNetLemmatizer from NLTK package, used to extract root words from the sentences.\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I've done the basic preprocessing.\n",
    "\n",
    "1. Given data is comma-seperated quoted strings.\n",
    "2. Data has no column names on the head.\n",
    "3. I figured out that, in each entry, first string related to the sentiment attached to that entry. It's value is in `0,2,4` indicating `negative, neutral, positive` respectively.\n",
    "4. There are other strings for each entry describing the user, timestamp, etc. \n",
    "5. Also I figured, in each entry, last string is the tweet itself on which the sentiment was determined.\n",
    "6. I've only taken two classes `0,4` for simplicity. Negative sentiment and positive sentiment. Assigned the one-hot classification vectors `[1,0]` representing negative sentiment and `[0,1]` representing positive sentiment.\n",
    "7. Then, I created a preprocessed file contianing the classification detail and the tweet itself seperated by triple colon (`:::`).\n",
    "\n",
    "Data is too large, so I decided to use buffering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_process(fin,fout):\n",
    "\tfin_path = \"/Users/tejasreddy9/Documents/PyNotebooks/Sentiment140/\"+fin\n",
    "\tfout_path = \"/Users/tejasreddy9/Documents/PyNotebooks/Sentiment140/\"+fout\n",
    "\toutfile = open(fout_path,'a')\n",
    "\twith open(fin_path, buffering=200000, encoding='latin-1') as f:\n",
    "\t\ttry:\n",
    "\t\t\tfor line in f:\n",
    "\t\t\t\tline = line.replace('\"','')\n",
    "\t\t\t\tinitial_polarity = line.split(',')[0]\n",
    "\t\t\t\tif initial_polarity == '0':\n",
    "\t\t\t\t\tinitial_polarity = [1,0]\n",
    "\t\t\t\telif initial_polarity == '4':\n",
    "\t\t\t\t\tinitial_polarity = [0,1]\n",
    "\n",
    "\t\t\t\ttweet = line.split(',')[-1]\n",
    "\t\t\t\toutline = str(initial_polarity)+':::'+tweet\n",
    "\t\t\t\toutfile.write(outline)\n",
    "\t\texcept Exception as e:\n",
    "\t\t\tprint(str(e))\n",
    "\t\tprint(\"Closed\",fin)\n",
    "\toutfile.close()\n",
    "\tprint(\"Closed\",fout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the below preprocessing only once. I'm commenting it out for now. As I'm using append, it will append to the bottom each time you run and causing duplicated data. Buffering is in 200,000 bytes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closed training.1600000.processed.noemoticon.csv\n",
      "Closed train_set.csv\n",
      "Closed testdata.manual.2009.06.14.csv\n",
      "Closed test_set.csv\n"
     ]
    }
   ],
   "source": [
    "# init_process('training.1600000.processed.noemoticon.csv','train_set.csv')\n",
    "# init_process('testdata.manual.2009.06.14.csv','test_set.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll create lexicon, and store it in pickle so that we keep track of code reusability. We'll load in the pickle whenever needed, it holds the lexicon for us. Run this when we have our `train_set.csv` is obtained from above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lexicon(fin):\n",
    "\tlexicon = []\n",
    "\tfin_path = \"/Users/tejasreddy9/Documents/PyNotebooks/Sentiment140/\"+fin\n",
    "\twith open(fin_path, 'r', buffering=100000, encoding='latin-1') as f:\n",
    "\t\ttry:\n",
    "\t\t\tcounter = 1\n",
    "\t\t\tcontent = ''\n",
    "\t\t\tfor line in f:\n",
    "\t\t\t\tcounter += 1\n",
    "\t\t\t\tif (counter/2500.0).is_integer():\n",
    "\t\t\t\t\ttweet = line.split(':::')[1]\n",
    "\t\t\t\t\tcontent += ' '+tweet\n",
    "\t\t\t\t\twords = word_tokenize(content)\n",
    "\t\t\t\t\twords = [lemmatizer.lemmatize(i) for i in words]\n",
    "\t\t\t\t\tlexicon = list(set(lexicon + words))\n",
    "\t\t\t\t\tprint(counter, len(lexicon))\n",
    "\n",
    "\t\texcept Exception as e:\n",
    "\t\t\tprint(str(e))\n",
    "\t\n",
    "\tprint(len(lexicon))\n",
    "\twith open('/Users/tejasreddy9/Documents/PyNotebooks/Sentiment140/lexicon-2500-2638.pickle','wb') as f:\n",
    "\t\tpickle.dump(lexicon,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to the preprocessing cell, we do run this only once to obtain our lexicon pickled. I'll comment this for now. Buffering is in 100,000 bytes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2500 26\n",
      "5000 32\n",
      "7500 41\n",
      "10000 63\n",
      "12500 85\n",
      "15000 89\n",
      "17500 91\n",
      "20000 108\n",
      "22500 113\n",
      "25000 120\n",
      "27500 128\n",
      "30000 141\n",
      "32500 159\n",
      "35000 173\n",
      "37500 177\n",
      "40000 185\n",
      "42500 198\n",
      "45000 212\n",
      "47500 224\n",
      "50000 240\n",
      "52500 253\n",
      "55000 266\n",
      "57500 267\n",
      "60000 269\n",
      "62500 280\n",
      "65000 284\n",
      "67500 291\n",
      "70000 299\n",
      "72500 306\n",
      "75000 321\n",
      "77500 329\n",
      "80000 336\n",
      "82500 345\n",
      "85000 356\n",
      "87500 358\n",
      "90000 361\n",
      "92500 374\n",
      "95000 388\n",
      "97500 399\n",
      "100000 402\n",
      "102500 412\n",
      "105000 418\n",
      "107500 425\n",
      "110000 429\n",
      "112500 438\n",
      "115000 448\n",
      "117500 450\n",
      "120000 453\n",
      "122500 460\n",
      "125000 465\n",
      "127500 466\n",
      "130000 468\n",
      "132500 470\n",
      "135000 480\n",
      "137500 490\n",
      "140000 496\n",
      "142500 500\n",
      "145000 507\n",
      "147500 514\n",
      "150000 528\n",
      "152500 532\n",
      "155000 536\n",
      "157500 540\n",
      "160000 548\n",
      "162500 556\n",
      "165000 560\n",
      "167500 562\n",
      "170000 566\n",
      "172500 571\n",
      "175000 580\n",
      "177500 583\n",
      "180000 588\n",
      "182500 594\n",
      "185000 597\n",
      "187500 600\n",
      "190000 607\n",
      "192500 611\n",
      "195000 621\n",
      "197500 624\n",
      "200000 628\n",
      "202500 630\n",
      "205000 632\n",
      "207500 641\n",
      "210000 644\n",
      "212500 648\n",
      "215000 651\n",
      "217500 659\n",
      "220000 674\n",
      "222500 684\n",
      "225000 689\n",
      "227500 694\n",
      "230000 698\n",
      "232500 700\n",
      "235000 706\n",
      "237500 714\n",
      "240000 717\n",
      "242500 721\n",
      "245000 722\n",
      "247500 725\n",
      "250000 727\n",
      "252500 732\n",
      "255000 741\n",
      "257500 743\n",
      "260000 747\n",
      "262500 754\n",
      "265000 757\n",
      "267500 762\n",
      "270000 765\n",
      "272500 767\n",
      "275000 771\n",
      "277500 776\n",
      "280000 776\n",
      "282500 777\n",
      "285000 781\n",
      "287500 782\n",
      "290000 786\n",
      "292500 793\n",
      "295000 797\n",
      "297500 799\n",
      "300000 805\n",
      "302500 808\n",
      "305000 812\n",
      "307500 815\n",
      "310000 818\n",
      "312500 820\n",
      "315000 825\n",
      "317500 828\n",
      "320000 830\n",
      "322500 833\n",
      "325000 834\n",
      "327500 844\n",
      "330000 848\n",
      "332500 852\n",
      "335000 855\n",
      "337500 859\n",
      "340000 861\n",
      "342500 864\n",
      "345000 870\n",
      "347500 879\n",
      "350000 881\n",
      "352500 892\n",
      "355000 895\n",
      "357500 903\n",
      "360000 906\n",
      "362500 912\n",
      "365000 915\n",
      "367500 916\n",
      "370000 924\n",
      "372500 937\n",
      "375000 941\n",
      "377500 945\n",
      "380000 947\n",
      "382500 948\n",
      "385000 952\n",
      "387500 956\n",
      "390000 964\n",
      "392500 966\n",
      "395000 972\n",
      "397500 980\n",
      "400000 982\n",
      "402500 989\n",
      "405000 990\n",
      "407500 993\n",
      "410000 996\n",
      "412500 998\n",
      "415000 1000\n",
      "417500 1006\n",
      "420000 1012\n",
      "422500 1014\n",
      "425000 1017\n",
      "427500 1023\n",
      "430000 1025\n",
      "432500 1027\n",
      "435000 1032\n",
      "437500 1035\n",
      "440000 1039\n",
      "442500 1044\n",
      "445000 1046\n",
      "447500 1048\n",
      "450000 1051\n",
      "452500 1061\n",
      "455000 1063\n",
      "457500 1064\n",
      "460000 1069\n",
      "462500 1077\n",
      "465000 1078\n",
      "467500 1080\n",
      "470000 1082\n",
      "472500 1085\n",
      "475000 1089\n",
      "477500 1092\n",
      "480000 1093\n",
      "482500 1102\n",
      "485000 1106\n",
      "487500 1109\n",
      "490000 1113\n",
      "492500 1120\n",
      "495000 1124\n",
      "497500 1126\n",
      "500000 1128\n",
      "502500 1134\n",
      "505000 1135\n",
      "507500 1147\n",
      "510000 1148\n",
      "512500 1151\n",
      "515000 1159\n",
      "517500 1162\n",
      "520000 1166\n",
      "522500 1168\n",
      "525000 1170\n",
      "527500 1176\n",
      "530000 1178\n",
      "532500 1186\n",
      "535000 1194\n",
      "537500 1196\n",
      "540000 1201\n",
      "542500 1202\n",
      "545000 1204\n",
      "547500 1209\n",
      "550000 1215\n",
      "552500 1218\n",
      "555000 1221\n",
      "557500 1225\n",
      "560000 1229\n",
      "562500 1233\n",
      "565000 1236\n",
      "567500 1238\n",
      "570000 1240\n",
      "572500 1240\n",
      "575000 1245\n",
      "577500 1251\n",
      "580000 1260\n",
      "582500 1264\n",
      "585000 1266\n",
      "587500 1275\n",
      "590000 1276\n",
      "592500 1278\n",
      "595000 1279\n",
      "597500 1285\n",
      "600000 1290\n",
      "602500 1296\n",
      "605000 1299\n",
      "607500 1303\n",
      "610000 1303\n",
      "612500 1309\n",
      "615000 1316\n",
      "617500 1317\n",
      "620000 1320\n",
      "622500 1321\n",
      "625000 1323\n",
      "627500 1323\n",
      "630000 1324\n",
      "632500 1329\n",
      "635000 1333\n",
      "637500 1335\n",
      "640000 1337\n",
      "642500 1337\n",
      "645000 1340\n",
      "647500 1344\n",
      "650000 1344\n",
      "652500 1351\n",
      "655000 1355\n",
      "657500 1357\n",
      "660000 1357\n",
      "662500 1360\n",
      "665000 1362\n",
      "667500 1369\n",
      "670000 1377\n",
      "672500 1384\n",
      "675000 1392\n",
      "677500 1397\n",
      "680000 1407\n",
      "682500 1409\n",
      "685000 1412\n",
      "687500 1414\n",
      "690000 1422\n",
      "692500 1428\n",
      "695000 1434\n",
      "697500 1437\n",
      "700000 1439\n",
      "702500 1442\n",
      "705000 1448\n",
      "707500 1455\n",
      "710000 1463\n",
      "712500 1473\n",
      "715000 1480\n",
      "717500 1482\n",
      "720000 1494\n",
      "722500 1498\n",
      "725000 1504\n",
      "727500 1512\n",
      "730000 1518\n",
      "732500 1522\n",
      "735000 1523\n",
      "737500 1524\n",
      "740000 1526\n",
      "742500 1527\n",
      "745000 1531\n",
      "747500 1537\n",
      "750000 1542\n",
      "752500 1547\n",
      "755000 1548\n",
      "757500 1550\n",
      "760000 1555\n",
      "762500 1559\n",
      "765000 1562\n",
      "767500 1567\n",
      "770000 1569\n",
      "772500 1570\n",
      "775000 1575\n",
      "777500 1577\n",
      "780000 1579\n",
      "782500 1588\n",
      "785000 1590\n",
      "787500 1594\n",
      "790000 1596\n",
      "792500 1596\n",
      "795000 1598\n",
      "797500 1603\n",
      "800000 1608\n",
      "802500 1615\n",
      "805000 1617\n",
      "807500 1622\n",
      "810000 1626\n",
      "812500 1629\n",
      "815000 1633\n",
      "817500 1634\n",
      "820000 1644\n",
      "822500 1646\n",
      "825000 1652\n",
      "827500 1655\n",
      "830000 1657\n",
      "832500 1659\n",
      "835000 1660\n",
      "837500 1663\n",
      "840000 1664\n",
      "842500 1665\n",
      "845000 1665\n",
      "847500 1666\n",
      "850000 1679\n",
      "852500 1685\n",
      "855000 1689\n",
      "857500 1693\n",
      "860000 1695\n",
      "862500 1700\n",
      "865000 1701\n",
      "867500 1703\n",
      "870000 1704\n",
      "872500 1710\n",
      "875000 1714\n",
      "877500 1715\n",
      "880000 1716\n",
      "882500 1718\n",
      "885000 1721\n",
      "887500 1725\n",
      "890000 1727\n",
      "892500 1731\n",
      "895000 1735\n",
      "897500 1742\n",
      "900000 1742\n",
      "902500 1746\n",
      "905000 1749\n",
      "907500 1750\n",
      "910000 1751\n",
      "912500 1758\n",
      "915000 1761\n",
      "917500 1764\n",
      "920000 1778\n",
      "922500 1781\n",
      "925000 1787\n",
      "927500 1799\n",
      "930000 1803\n",
      "932500 1806\n",
      "935000 1807\n",
      "937500 1809\n",
      "940000 1810\n",
      "942500 1813\n",
      "945000 1819\n",
      "947500 1820\n",
      "950000 1820\n",
      "952500 1821\n",
      "955000 1823\n",
      "957500 1830\n",
      "960000 1836\n",
      "962500 1840\n",
      "965000 1842\n",
      "967500 1847\n",
      "970000 1852\n",
      "972500 1859\n",
      "975000 1861\n",
      "977500 1870\n",
      "980000 1874\n",
      "982500 1876\n",
      "985000 1878\n",
      "987500 1879\n",
      "990000 1886\n",
      "992500 1888\n",
      "995000 1895\n",
      "997500 1896\n",
      "1000000 1899\n",
      "1002500 1900\n",
      "1005000 1901\n",
      "1007500 1913\n",
      "1010000 1915\n",
      "1012500 1918\n",
      "1015000 1922\n",
      "1017500 1926\n",
      "1020000 1928\n",
      "1022500 1930\n",
      "1025000 1931\n",
      "1027500 1932\n",
      "1030000 1939\n",
      "1032500 1942\n",
      "1035000 1944\n",
      "1037500 1949\n",
      "1040000 1953\n",
      "1042500 1958\n",
      "1045000 1961\n",
      "1047500 1970\n",
      "1050000 1972\n",
      "1052500 1975\n",
      "1055000 1978\n",
      "1057500 1978\n",
      "1060000 1980\n",
      "1062500 1986\n",
      "1065000 1988\n",
      "1067500 1990\n",
      "1070000 1994\n",
      "1072500 1995\n",
      "1075000 1997\n",
      "1077500 1997\n",
      "1080000 1997\n",
      "1082500 2000\n",
      "1085000 2007\n",
      "1087500 2017\n",
      "1090000 2020\n",
      "1092500 2024\n",
      "1095000 2028\n",
      "1097500 2032\n",
      "1100000 2038\n",
      "1102500 2043\n",
      "1105000 2048\n",
      "1107500 2054\n",
      "1110000 2057\n",
      "1112500 2058\n",
      "1115000 2060\n",
      "1117500 2065\n",
      "1120000 2068\n",
      "1122500 2072\n",
      "1125000 2080\n",
      "1127500 2086\n",
      "1130000 2087\n",
      "1132500 2089\n",
      "1135000 2089\n",
      "1137500 2094\n",
      "1140000 2099\n",
      "1142500 2101\n",
      "1145000 2105\n",
      "1147500 2109\n",
      "1150000 2113\n",
      "1152500 2114\n",
      "1155000 2120\n",
      "1157500 2124\n",
      "1160000 2126\n",
      "1162500 2132\n",
      "1165000 2133\n",
      "1167500 2146\n",
      "1170000 2153\n",
      "1172500 2159\n",
      "1175000 2161\n",
      "1177500 2164\n",
      "1180000 2164\n",
      "1182500 2165\n",
      "1185000 2167\n",
      "1187500 2167\n",
      "1190000 2174\n",
      "1192500 2178\n",
      "1195000 2181\n",
      "1197500 2185\n",
      "1200000 2186\n",
      "1202500 2190\n",
      "1205000 2196\n",
      "1207500 2197\n",
      "1210000 2199\n",
      "1212500 2202\n",
      "1215000 2205\n",
      "1217500 2205\n",
      "1220000 2212\n",
      "1222500 2216\n",
      "1225000 2219\n",
      "1227500 2223\n",
      "1230000 2225\n",
      "1232500 2228\n",
      "1235000 2229\n",
      "1237500 2229\n",
      "1240000 2231\n",
      "1242500 2233\n",
      "1245000 2233\n",
      "1247500 2234\n",
      "1250000 2237\n",
      "1252500 2239\n",
      "1255000 2242\n",
      "1257500 2246\n",
      "1260000 2250\n",
      "1262500 2254\n",
      "1265000 2257\n",
      "1267500 2260\n",
      "1270000 2261\n",
      "1272500 2261\n",
      "1275000 2262\n",
      "1277500 2265\n",
      "1280000 2272\n",
      "1282500 2273\n",
      "1285000 2273\n",
      "1287500 2277\n",
      "1290000 2281\n",
      "1292500 2284\n",
      "1295000 2285\n",
      "1297500 2286\n",
      "1300000 2287\n",
      "1302500 2289\n",
      "1305000 2291\n",
      "1307500 2293\n",
      "1310000 2297\n",
      "1312500 2297\n",
      "1315000 2299\n",
      "1317500 2303\n",
      "1320000 2304\n",
      "1322500 2306\n",
      "1325000 2310\n",
      "1327500 2314\n",
      "1330000 2318\n",
      "1332500 2323\n",
      "1335000 2327\n",
      "1337500 2327\n",
      "1340000 2330\n",
      "1342500 2330\n",
      "1345000 2335\n",
      "1347500 2339\n",
      "1350000 2344\n",
      "1352500 2345\n",
      "1355000 2345\n",
      "1357500 2348\n",
      "1360000 2350\n",
      "1362500 2351\n",
      "1365000 2356\n",
      "1367500 2358\n",
      "1370000 2361\n",
      "1372500 2364\n",
      "1375000 2368\n",
      "1377500 2370\n",
      "1380000 2374\n",
      "1382500 2375\n",
      "1385000 2376\n",
      "1387500 2380\n",
      "1390000 2384\n",
      "1392500 2386\n",
      "1395000 2387\n",
      "1397500 2390\n",
      "1400000 2393\n",
      "1402500 2397\n",
      "1405000 2397\n",
      "1407500 2398\n",
      "1410000 2404\n",
      "1412500 2407\n",
      "1415000 2413\n",
      "1417500 2414\n",
      "1420000 2416\n",
      "1422500 2421\n",
      "1425000 2422\n",
      "1427500 2423\n",
      "1430000 2426\n",
      "1432500 2427\n",
      "1435000 2428\n",
      "1437500 2431\n",
      "1440000 2432\n",
      "1442500 2435\n",
      "1445000 2438\n",
      "1447500 2438\n",
      "1450000 2440\n",
      "1452500 2443\n",
      "1455000 2446\n",
      "1457500 2452\n",
      "1460000 2455\n",
      "1462500 2456\n",
      "1465000 2458\n",
      "1467500 2459\n",
      "1470000 2468\n",
      "1472500 2470\n",
      "1475000 2473\n",
      "1477500 2480\n",
      "1480000 2483\n",
      "1482500 2491\n",
      "1485000 2498\n",
      "1487500 2499\n",
      "1490000 2500\n",
      "1492500 2501\n",
      "1495000 2504\n",
      "1497500 2509\n",
      "1500000 2513\n",
      "1502500 2515\n",
      "1505000 2516\n",
      "1507500 2517\n",
      "1510000 2518\n",
      "1512500 2523\n",
      "1515000 2526\n",
      "1517500 2528\n",
      "1520000 2529\n",
      "1522500 2531\n",
      "1525000 2536\n",
      "1527500 2542\n",
      "1530000 2543\n",
      "1532500 2553\n",
      "1535000 2555\n",
      "1537500 2558\n",
      "1540000 2561\n",
      "1542500 2562\n",
      "1545000 2564\n",
      "1547500 2571\n",
      "1550000 2573\n",
      "1552500 2574\n",
      "1555000 2579\n",
      "1557500 2579\n",
      "1560000 2582\n",
      "1562500 2588\n",
      "1565000 2590\n",
      "1567500 2591\n",
      "1570000 2595\n",
      "1572500 2599\n",
      "1575000 2601\n",
      "1577500 2603\n",
      "1580000 2605\n",
      "1582500 2606\n",
      "1585000 2609\n",
      "1587500 2614\n",
      "1590000 2618\n",
      "1592500 2625\n",
      "1595000 2626\n",
      "1597500 2632\n",
      "1600000 2638\n",
      "2638\n"
     ]
    }
   ],
   "source": [
    "# create_lexicon('train_set.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We got a large lexicon-set having `2638` unique lexicon determined by NLTK. This indicates that we need our neural network's initial layer to be having `2638` nodes on as input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to convert our training set into feature vectors, so that we can train our models. \n",
    "\n",
    "As we have observed that for nearly 3MB data we used in the first attempt, vectors produced are of complete size 140MB. But, the data we have is 113MB intially, this could scale upto 20GB. \n",
    "\n",
    "DO NOT RUN THIS ON CPU. WE WILL DO THIS INLINE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_vec(fin,fout,lexicon_pickle):\n",
    "\tfin_path = \"/Users/tejasreddy9/Documents/PyNotebooks/Sentiment140/\"+fin\n",
    "\tfout_path = \"/Users/tejasreddy9/Documents/PyNotebooks/Sentiment140/\"+fout\n",
    "\tlexicon_path = \"/Users/tejasreddy9/Documents/PyNotebooks/Sentiment140/\"+lexicon_pickle\n",
    "\twith open(lexicon_path,'rb') as f:\n",
    "\t\tlexicon = pickle.load(f)\n",
    "\toutfile = open(fout_path,'a')\n",
    "\twith open(fin_path, buffering=20000, encoding='latin-1') as f:\n",
    "\t\tcounter = 0\n",
    "\t\tfor line in f:\n",
    "\t\t\tcounter +=1\n",
    "\t\t\tlabel = line.split(':::')[0]\n",
    "\t\t\ttweet = line.split(':::')[1]\n",
    "\t\t\tcurrent_words = word_tokenize(tweet.lower())\n",
    "\t\t\tcurrent_words = [lemmatizer.lemmatize(i) for i in current_words]\n",
    "\n",
    "\t\t\tfeatures = np.zeros(len(lexicon))\n",
    "\n",
    "\t\t\tfor word in current_words:\n",
    "\t\t\t\tif word.lower() in lexicon:\n",
    "\t\t\t\t\tindex_value = lexicon.index(word.lower())\n",
    "\t\t\t\t\t# OR DO +=1, test both\n",
    "\t\t\t\t\tfeatures[index_value] += 1\n",
    "\n",
    "\t\t\tfeatures = list(features)\n",
    "\t\t\toutline = str(features)+'::'+str(label)+'\\n'\n",
    "\t\t\toutfile.write(outline)\n",
    "\n",
    "\t\tprint(counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shuffle the training data, as per the norms of neutral networks. Otherwise, it tries to do the unnecessary re-adjustment makes the weigths so large layer by layer, and at some threshold point, subsequent layers will have negligible contribution. Hence, shuffling is mandatory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_data(fin):\n",
    "\tdf = pd.read_csv(fin, error_bad_lines=False)\n",
    "\tdf = df.iloc[np.random.permutation(len(df))]\n",
    "\tprint(df.head())\n",
    "\tdf.to_csv('train_set_shuffled.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run this also only once, we just meant to shuffle it once before we go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffle_data('train_set.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For, testing dataset also, we'll create a pickle so that we can load it whenever needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_test_data_pickle(fin):\n",
    "\n",
    "\tfeature_sets = []\n",
    "\tlabels = []\n",
    "\tcounter = 0\n",
    "\twith open(fin, buffering=20000) as f:\n",
    "\t\tfor line in f:\n",
    "\t\t\ttry:\n",
    "\t\t\t\tfeatures = list(eval(line.split('::')[0]))\n",
    "\t\t\t\tlabel = list(eval(line.split('::')[1]))\n",
    "\n",
    "\t\t\t\tfeature_sets.append(features)\n",
    "\t\t\t\tlabels.append(label)\n",
    "\t\t\t\tcounter += 1\n",
    "\t\t\texcept:\n",
    "\t\t\t\tpass\n",
    "\tprint(counter)\n",
    "\tfeature_sets = np.array(feature_sets)\n",
    "\tlabels = np.array(labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This also run only once. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_test_data_pickle('processed-test-set.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
