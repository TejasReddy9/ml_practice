{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First Attempt towards sentiment analysis\n",
    "#### I've used the same model that I tried for MNIST dataset. I got 60% accuracy, proves that modification is to be done in order to observe improvisation. \n",
    "#### Inference: Need on work on bigger training data, maybe 1.6 million sentences. Or maybe somewhat different algorithm, tweak with hidden layers, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL']='2'\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's open the file which we pickled back in the day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/Users/tejasreddy9/Documents/PyNotebooks/sentiment_set.pickle\",\"rb\") as file:\n",
    "    train_x, train_y, test_x, test_y = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we loaded in the training and validation sets, let's do the same process as we have done to MNIST dataset.\n",
    "\n",
    "Now dimension will be `train_x[i]` initially. In the case of MNIST dataset, it was `784`(28*28)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_nodes_hl1 = 500\n",
    "n_nodes_hl2 = 500\n",
    "n_nodes_hl3 = 500\n",
    "\n",
    "n_classes = 2\n",
    "batch_size = 100\n",
    "\n",
    "initial_size = len(train_x[0])\n",
    "x = tf.placeholder('float', [None, initial_size])\n",
    "y = tf.placeholder('float')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Network model is the same with the core concept of feedforward-backpropagation (epoch) model.\n",
    "\n",
    "Let's use the core concept of deep learning, computations are done by setting weights for each connection of one in a layer to the other in the next layer. A bias term is added to ensure that the result is not always zero whenever data or weights is zero. If the result from a layer is all zero for certain entries, then it would be biased for that layer as the contribution of the following layers is very insignificant.\n",
    "\n",
    "This is the computation to be done for each layer. (weights * data) + biases\n",
    "\n",
    "Let's use random_normal function for generating random values. We use ReLU activation function on each layer.\n",
    "\n",
    "Feedforward model as the output of one layer is given as input to the next layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neural_network_model(data):\n",
    "    hidden_1_layer = {'weights':tf.Variable(tf.random_normal([initial_size, n_nodes_hl1])), \n",
    "                        'biases':tf.Variable(tf.random_normal([n_nodes_hl1]))}\n",
    "    hidden_2_layer = {'weights':tf.Variable(tf.random_normal([n_nodes_hl1, n_nodes_hl2])), \n",
    "                        'biases':tf.Variable(tf.random_normal([n_nodes_hl2]))}\n",
    "    hidden_3_layer = {'weights':tf.Variable(tf.random_normal([n_nodes_hl2, n_nodes_hl3])), \n",
    "                        'biases':tf.Variable(tf.random_normal([n_nodes_hl3]))}\n",
    "    output_layer = {'weights':tf.Variable(tf.random_normal([n_nodes_hl3, n_classes])), \n",
    "                        'biases':tf.Variable(tf.random_normal([n_classes]))}\n",
    "    \n",
    "    l1 = tf.add(tf.matmul(data,hidden_1_layer['weights']), hidden_1_layer['biases'])\n",
    "    l1 = tf.nn.relu(l1)  # activation function \t\n",
    "    \n",
    "    l2 = tf.add(tf.matmul(l1,hidden_2_layer['weights']), hidden_2_layer['biases'])\n",
    "    l2 = tf.nn.relu(l2)\n",
    "    \n",
    "    l3 = tf.add(tf.matmul(l2,hidden_3_layer['weights']), hidden_3_layer['biases'])\n",
    "    l3 = tf.nn.relu(l3)\n",
    "    \n",
    "    output = tf.add(tf.matmul(l3,output_layer['weights']), output_layer['biases'])\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, given that our model is implemented, let's train over training data.\n",
    "\n",
    "The cost function we consider here is reduced mean of the softmax cross entropy results over the labels.\n",
    "\n",
    "So, now that done, we go into running our cycles. Each cycle (epoch) consists of a feedforward stride and a backpropagtion result used to improve the weights internally. Also for each epoch, we calculate corresponding loss after using AdamOptimizer to reduce minimize the cost function. We'll display the progress.\n",
    "\n",
    "By comparing which predictions actual match, accuracy is calculated by reduced mean. Now, let's evaluate over the completely unknown test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_neural_network(x):\n",
    "    prediction = neural_network_model(x)\n",
    "    cost = tf.reduce_mean( tf.nn.softmax_cross_entropy_with_logits(logits=prediction, labels=y) )\n",
    "    optimizer = tf.train.AdamOptimizer().minimize(cost)\n",
    "    \n",
    "    hm_epochs = 10\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        for epoch in range(hm_epochs):\n",
    "            epoch_loss = 0\n",
    "            for i in range(0,len(train_x),batch_size):\n",
    "                batch_x = np.array(train_x[i:i+batch_size])\n",
    "                batch_y = np.array(train_y[i:i+batch_size])\n",
    "                _, c = sess.run([optimizer, cost], feed_dict={x: batch_x, y: batch_y})\n",
    "                epoch_loss += c\n",
    "            print('Epoch', epoch, 'completed out of',hm_epochs,'loss:',epoch_loss)\n",
    "\n",
    "        correct = tf.equal(tf.argmax(prediction, 1), tf.argmax(y, 1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct, 'float'))\n",
    "        print('Accuracy:',accuracy.eval({x:test_x, y:test_y}))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run that. It goes through each epoch and we can observe the model building by reducing the cost function, self adjusting the weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-7-5874c2af50eb>:3: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n",
      "\n",
      "Epoch 0 completed out of 10 loss: 228661.16357421875\n",
      "Epoch 1 completed out of 10 loss: 116298.01574707031\n",
      "Epoch 2 completed out of 10 loss: 71950.77996826172\n",
      "Epoch 3 completed out of 10 loss: 48113.845764160156\n",
      "Epoch 4 completed out of 10 loss: 33231.18165588379\n",
      "Epoch 5 completed out of 10 loss: 31761.54453277588\n",
      "Epoch 6 completed out of 10 loss: 30378.60150909424\n",
      "Epoch 7 completed out of 10 loss: 15416.030141830444\n",
      "Epoch 8 completed out of 10 loss: 10187.167241096497\n",
      "Epoch 9 completed out of 10 loss: 8538.833990573883\n",
      "Accuracy: 0.57973737\n"
     ]
    }
   ],
   "source": [
    "train_neural_network(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not much impressive, I must try another approach or better dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
